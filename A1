[6] 1. Describe an efficient algorithm for the following problem. 
The input is an instance for the Stable Matching problem (n men, n women, and their preference lists). 
The output should be “yes” if that instance has only one stable matching. 
Otherwise the output should be “no”.
 (You can use any theorems mentioned in the lecture or written in the textbooks.) 
Input:  
- integer n, the number of men and women  
- preference list for each man and woman

Output: 
- true if only one stable matching; false otherwise

Procedure: 
-> Run the StableMatching algorithm twice (men proposing, then women proposing)
-> If returns of both algorithms are equal, output true (false if otherwise)

Theorem & Proofs: 
	-> StableMatching Algorithm outputs stable matching (proven inclass)
	-> (man-optimality leads to woman-pessimality: 
-> If women are proposing, they get their best choice. If men are proposing, they get their best choice (which may not necessarily be the same) 
->  If a woman is free, there is at least one man to whom she hasn’t proposed (proven in class)






[12] 2. For all n, design an instance of the Stable Matching problem such that the Proposal Algorithm will execute for Ω(n^2 ) iterations when provided that instance as input. (You also need to describe the sequence of proposals that the algorithm should make.) 

Theorem: ForAll n, ThereExists an instance such that algorithm will run in  Omega(n^2)
PROOF:	
	By Induction
	BASE CASE:
	If n = 2:
		Mens Preferences:
m1: w1, w2, 
m2: w1, w2, 

Women’s Preferences:
w1: m2, m1, 
w2: m2, m1,

	In this case: 
		M1 -> w1
		M2 -> w1 (m1 dropped)
		M1 -> w2
	Overall, 3 matches (or k = n + (n-1) )

	INDUCTION STEP: 
	If n = 3:
		Mens Preferences:
m1: w1, w2, w3
m2: w1, w2, w3
m3: w1, w2, w3

Women’s Preferences:
w1: m3, m2, m1, 
w2: m3, m2, m1,
w3: m3, m2, m1,

	In this case: 
		M1 -> w1
		M2 -> w1 (m1 dropped)
		M3 -> w1 (m2 dropped)		
		M1 -> w2
		M2 -> w2 (m1 dropped)
		M1 -> w3
Overall, 6 matches (or k = n + (n-1) + (n-2) )

	Essentially, 
		If the preferences are set as so:
			M1: w1, …, wn
			..
			Mn: w1, ... , wn
			W1: mn, … , m1
			.. 
			Wn: mn, … , m1

		The number of proposals will be:
	sum(i = 1, n) { 1+2+3+ … n }
	sum(i = 1, n) { n(n+1) / 2 }
-> This will be Theta(n^2), which is also Omega(n^2)

Therefore, the algorithm will run for Omega(n^2)	


[12] 3. You are doing stress-testing on various models of glass jars to determine the height from which they can be dropped and still not break. The setup for this experiment, on a particular type of jar, is as follows. 
You have a ladder with n rungs, and you want to find the highest rung from which you can drop a copy of the jar and not have it break. We call this the highest safe rung. 
It might be natural to try binary search: drop a jar from the middle rung, see if it breaks, and then recursively try from rung n/4 or 3n/4 depending on the outcome. While this algorithm will require the fewest tests, it may also result in many broken jars. 


[3] (a) How many jars might you end up breaking, in the worst case?
	T(n/2) = n/4
T(n/4) = n/8
T(n) = n / 2^k = 1
 N = 2^k
log(n) = k
Therefore, in the worst  case we’ll end up breaking log(n) jars

If your primary goal were to conserve jars, on the other hand, you could try a different strategy. 
Start by dropping a jar from the first rung, then the second rung, etc. 
In this way, you break at most one jar. 
Unfortunately, you may also need n attempts. 
So there seems to be a trade-off: the more jars you are willing to break, the fewer tries you will need. 

[6] (b) Your boss is really cheap, but he is also too impatient to let you make n attempts. So he gives you 2 jars. Describe a strategy for finding the highest safe rung that requires you to drop a jar at most f(n) times, where f(n) is a function that is o(n). (So, for example, f(n) = n/2 is not acceptable.) 

	HSR = saferung = ??
	n = max # of rungs

1st jar: segment ladder into sections
2nd jar: linear search

What we need: # of sections * # of runs per section = n
	1st jar: # of sections
	2nd jar: # of runs per section
		Sum must be o(n) AND product must be  = n
	
Let a =  # of sections
	Let b =  # of rungs per section
	
	We need to satisfy these requirements:
	a + b is o(n)
	a * b = n

	If n^2:
		n = a, n = b 
-> n * n = n^2	(yes)
-> n + n = 2n, 2n =  o(n^2)	(no)
		
	If a & b = sqrt(n):
		sqrt(n) * sqrt(n) = n			-> a * b = n: yes
		sqrt(n) + sqrt(n) = 2*sqrt(n) 		-> a + b in o(n): yes
	
THEREFORE: 
	Overall procedure:
	Let n = total # of rungs
	Let a = # of sections we segment the ladder into
	(segment the ladder into a sections)
	Let b =  # of runs per section
	a = sqrt(n)
	b = sqrt(n)
	For each section a:
		- drop 1st jar at some point within section
		- if jar breaks:
			Let a* = section we’re going to iterate to find HSR
			ForEach rung in the section a*, from 0 to b-1:		
				- Drop 2nd jar 
				- if 2nd jar breaks:
					This is the HSR: return the value
	

[3] (c) Analyze the worst-case number of attempts of your algorithm from part (b).
Let n = total # of rungs in ladder
Worst Possible Case:
	If HSR = very last rung:
		1st jar: all sections have been tested 		
			= sqrt(n) sections
		2nd jar: all rungs in the section are tested
			= sqrt(n) rungs

	THerefore:
	2*sqrt(n) attempts


[12] 4. Consider the following basic problem: you are given an array A of size n, and you want to generate a two-dimensional n × n array B such that




That is, B[i, j] contains the sum of the elements from A[i] to A[j] (unless j < i). Here is a simple algorithm that achieves this:


[2] (a) Using O notation, give as close an upper bound as you can for the running time of algorithm ComputeMatrix, as a function of n. 
Algorithm ComputeMatrix 
for i ← 1 to n do 
for j ← 1 to n do 
if i ≤ j then 
B[i,j] ← the sum of the elements A[i], A[i+1], ..., A[j] 
else 
B[i,j] ← 0

	for i ← 1 to n do =>>> n
for j ← 1 to n do  =>>>>> n
(Cost C1)
	B[i,j] ← the sum of the elements A[i], A[i+1], ..., A[j] 
			=>>>> n
(Cost C2)

n (n *(Cost C1) * n + (Cost C2)) => n(n*n + 1) => n^3	

	This is O(n^3)

[8] (b) Although algorithm ComputeMatrix is the most natural one to solve the problem, it is not the most efficient. Give a different algorithm to solve this problem whose running time is a factor of n faster than that of algorithm ComputeMatrix. 

	Let: 
	A = [random array of ints]
B = [cumulative sum of random array:]
B[i=0, j=0] = a[0]
	ForEach i (0 to  A’s size - 1):
		ForEach j (1 to B’s size - 1):
if i <= j: 	
 B[i, j] = b[i, j-1] + a[j]

Ex:
A = [ 1, 2, 2, 4, 5]
j	
B = [ 1, 3, 5, 9, 14]
	  i    [ x, 2, 4, 8, 13 ]
	       [ x, _, _, _, _ ]
	       [ …	     ]

[2] (c) Using Θ notation, write down the running time of your algorithm from part (b). You must justify your answer.

THe ALgorithm:

	ForEach i (0 to  A’s size - 1):			=> n iterations
		ForEach j (1 to B’s size - 1):		=> n iterations
if i <= j: 	
 B[i, j] = b[i, j-1] + a[j]	=> constant

	Therefore, 
		n * (n + C)	=> n^2
	
	Runtime: O(n^2)

